{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Collaborative_filtering (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4eEyx4pV2CO"
      },
      "source": [
        "import numpy as np ## version 1.17\n",
        "import pandas as pd \n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcuJkYPcWaOG",
        "outputId": "fc547a30-6a59-4ae2-fb3e-ccb53af5ed62"
      },
      "source": [
        "!pip install pickle5\n",
        "import pickle5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 6.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp36-cp36m-linux_x86_64.whl size=218599 sha256=3899b3ef98e7bf2b0c9dc24c5cabd7645026ced79e6ec496feb714984fe6f6a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQFSwCAMV2CQ"
      },
      "source": [
        "## Couldn't import the pickle files so using the dataset directly ##\n",
        "review = pd.read_csv('/N/u/abbane/Carbonate/Downloads/yelp_academic_dataset_review.csv')\n",
        "business = pd.read_csv('/N/u/abbane/Carbonate/Downloads/yelp_academic_dataset_business.csv')\n",
        "users = pd.read_csv('/N/u/abbane/Carbonate/Downloads/yelp_academic_dataset_user.csv', encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScPNj4IjV2CQ"
      },
      "source": [
        "business = business[business.columns.drop(list(business.filter(regex='attributes')))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J90wlXNWV2CQ"
      },
      "source": [
        "### preprocessing from Vidit's code since I couldnt import pickle files ##\n",
        "\n",
        "doc_result = docx2python(\"/N/u/abbane/Carbonate/Search/Categories.docx\")\n",
        "categories_to_subcategories = {}\n",
        "k = None\n",
        "for i in doc_result.body[0][0][0]:\n",
        "    if len(re.findall('\\t', i)) == 0:\n",
        "        k = ' '.join(re.findall(r'\\w+', i))\n",
        "        categories_to_subcategories[k] = [k]\n",
        "    \n",
        "    categories_to_subcategories[k].append(' '.join(re.findall(r'\\w+', i)))\n",
        "\n",
        "subcategories_to_categories = {}\n",
        "for k, v in categories_to_subcategories.items():\n",
        "    for x in set(v):\n",
        "        subcategories_to_categories.setdefault(x.lower(), []).append(k.lower())\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/questions/54481198/python-match-multiple-substrings-in-a-string\n",
        "def clean_categories(string):\n",
        "    if type(string) is str:\n",
        "        categories = []\n",
        "        for key, values in categories_to_subcategories.items():\n",
        "            pattern = r'\\b({})\\b'.format('|'.join(map(re.escape, values)))\n",
        "            matches = set(map(str.lower, re.findall(pattern, string, re.IGNORECASE)))\n",
        "            categories.extend(list(filter(None, [x.lower() for x in values if x.lower() in matches])))\n",
        "        return list(set(categories))\n",
        "    else:\n",
        "        return string\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#business = pd.read_csv('yelp_dataset/pandas_business_1.csv')\n",
        "\n",
        "# Dropping hours and attributes as their sub dictionaries are already added as features.\n",
        "#business.drop(['Unnamed: 0', 'hours', 'attributes'], axis=1, inplace=True)\n",
        "business = business[business['is_open'] == 1]\n",
        "\n",
        "business['rating_category'] = business['stars'].apply(lambda x: 'low' if x < 3 else ('medium' if x < 4 else 'high'))\n",
        "\n",
        "business = business[business['state'].isin(business.groupby(by='state')['business_id'].count().sort_values(ascending=False).nlargest(7).keys().values)]\n",
        "\n",
        "business['cleaned_categories'] = business['categories'].apply(clean_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSNOldMUV2CQ"
      },
      "source": [
        "business['cleaned_categories'] = business['cleaned_categories'].apply(lambda x: x if type(x) is list else math.nan)\n",
        "business['parent_categories'] = business['cleaned_categories'].apply(lambda x: list(set([subcategories_to_categories[k][0] for k in x])) if type(x) is list else x)\n",
        "\n",
        "selected_parent_categories = business[['parent_categories', 'business_id']].explode('parent_categories').reset_index()[['parent_categories', 'business_id']].groupby(\n",
        "    by='parent_categories')['business_id'].count().nlargest(2).index.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmfLb0ppV2CQ"
      },
      "source": [
        "business = business[business['business_id'].isin(\n",
        "    pd.concat([business[['parent_categories', 'business_id']].explode('parent_categories').reset_index()[['parent_categories', 'business_id']].groupby(\n",
        "        by='parent_categories').get_group(name)['business_id'] for name in selected_parent_categories]))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbs2OvjVV2CQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWqjDmIkV2CQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNa7nJijV2CR"
      },
      "source": [
        "## User data.. need to figure out utf8 encoding..need to fix this  ##\n",
        "#users['year'] = pd.DatetimeIndex(pd.to_datetime(users['yelping_since'], format='%Y-%m-%d %H:%M:%S')).year\n",
        "#users = users[users['year'] >= 2010]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2YkHEddGV2CR"
      },
      "source": [
        "review = review[['user_id','business_id','stars']]\n",
        "business = business[['business_id','name','city','stars']]\n",
        "\n",
        "user_review = pd.merge(review,business,how='inner',on='business_id')\n",
        "user_review.columns = ['user_id','business_id','user_stars','name','city','business_stars']\n",
        "\n",
        "## Saving the file to reuse it later ##\n",
        "user_review.to_csv('/N/u/abbane/Carbonate/Downloads/user_review_mat_food_rest.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIY28plLWGoQ"
      },
      "source": [
        "## Importing preprocessed files directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bt87MSLWvnF",
        "outputId": "73054b78-c044-4613-dc42-689acfb3078c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gzfmd63V2CR"
      },
      "source": [
        "### Import user reviews directly instead of business and reviews separetly everytime ###\n",
        "# user_review = pd.read_csv('/N/u/abbane/Carbonate/Downloads/user_review_mat.csv')\n",
        "# user_review = pd.read_csv(\"/content/drive/My Drive/archive/user_review_mat_food_rest_v2.csv/\")\n",
        "with open('/content/drive/My Drive/archive/user_review_mat_v3.pkl','rb') as file:\n",
        "    user_review = pickle5.load(file)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSIrw85QV2CR",
        "outputId": "65ff40c6-d2b1-41da-e4fe-291b8e7c7116"
      },
      "source": [
        "## Top businesses overall ##\n",
        "user_review.groupby(['business_id','name'])['user_stars'].mean().sort_values(ascending=False).head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "business_id             name                 \n",
              "T6rvTatc7QQMd-uvDr5OZQ  Headfoneshop             5.0\n",
              "Jzd5ZpOxzjG4_pxX6C6rBQ  Las Vegas Electrician    5.0\n",
              "VVxf_sOaHqLzFLDMDYUpVA  Quick Stop               5.0\n",
              "VTOvXMJvmG1LNmYMdQ3eNw  Q. Contrary              5.0\n",
              "JyTMXTaqQ1WepVH5Fuv6Aw  DePry Acupuncture        5.0\n",
              "Name: user_stars, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYdPBrjyV2CR"
      },
      "source": [
        "#user_review.pivot_table(values='user_stars', index='user_id', columns='name', fill_value=0).head()\n",
        "## sorting the user reviews so I can break ot down for pivot ##\n",
        "user_review = user_review.sort_values('name')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc-JpF2tV2CR"
      },
      "source": [
        "## remove users having less than 50 ratings to avoid coldstarts ##\n",
        "## need to try different 'n' and check accuracy ##\n",
        "users = user_review[['user_id','business_id']].groupby('user_id').count().sort_values(by='business_id',ascending=False).reset_index()\n",
        "users = users[users.business_id >= 50]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkecNwwzV2CR"
      },
      "source": [
        "## joining with the user_review table to filter out the users with more than 50 reviews ##\n",
        "user_review = pd.merge(user_review,users['user_id'],how='inner', on='user_id')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMExWc4WV2CS"
      },
      "source": [
        "## Trying the surprise library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu3m-kZwXGI9",
        "outputId": "9d9e7222-90b7-45e3-8f63-d677c140e19f"
      },
      "source": [
        "!pip install surprise"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting surprise\n",
            "  Downloading https://files.pythonhosted.org/packages/61/de/e5cba8682201fcf9c3719a6fdda95693468ed061945493dea2dd37c5618b/surprise-0.1-py2.py3-none-any.whl\n",
            "Collecting scikit-surprise\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/37/5d334adaf5ddd65da99fc65f6507e0e4599d092ba048f4302fe8775619e8/scikit-surprise-1.1.1.tar.gz (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp36-cp36m-linux_x86_64.whl size=1670911 sha256=433a26a566bb6053b0b939f31fae5f66b39f58269b516192c268e9b02eae035d\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/9c/3d/41b419c9d2aff5b6e2b4c0fc8d25c538202834058f9ed110d0\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnr5sT9dV2CS"
      },
      "source": [
        "from surprise import SVD\n",
        "from surprise import BaselineOnly\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "from surprise.model_selection import KFold\n",
        "from surprise.model_selection import GridSearchCV"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWQpdVBxV2CS"
      },
      "source": [
        "def get_top_n(predictions, n=10):\n",
        "    \"\"\"\n",
        "    This function returns the top n predictions for every user\n",
        "    \n",
        "    Parameters:\n",
        "    \n",
        "    predictions: ratings for every user predicted by the model\n",
        "     \n",
        "    n: Number of restaurants to be recommended default is 10\n",
        "    \n",
        "    Returns:\n",
        "    A dictionary with key as the user id and a list of businesses as the value\n",
        "    \"\"\"\n",
        "    # First map the predictions to each user.\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQgSysKAV2CS"
      },
      "source": [
        "def precision_recall_at_k(predictions, k=10, threshold=3):\n",
        "    \"\"\"\n",
        "    This function calculates precision and recall for the predicted ratings of all the users.\n",
        "    \n",
        "    Parameters: \n",
        "    \n",
        "    predictions: ratings for every user predicted by the model\n",
        "    \n",
        "    k: Value for k in Precision@k / Recall@k, to be evaluated for top k recommendations\n",
        "    \n",
        "    threshold: threshold to classify a review as positive or negative, default is 3\n",
        "    \n",
        "    Returns:\n",
        "    Two dictionaries one for precision and recall each. Both the metrics are evaluated at a user level.\n",
        "    \"\"\"\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "        \n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        \n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "        # Number of relevant and recommended items in top k\n",
        "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
        "\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
        "\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
        "\n",
        "    return precisions, recalls\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6Kcw2JpV2CS"
      },
      "source": [
        "## can improve this function  ##\n",
        "def calculate_ndcg(prediction,k, threshold):\n",
        "    \"\"\"\n",
        "    This function calculates binary NDCG for the predicted ratings of all the users. To use the NDCG in binary form, \n",
        "    we would be considering all the ratings below the threshold as non-relevant and all the ratings above the threshold\n",
        "    as relevant. NDCG will be calculated as:\n",
        "    (relevancy)/log2(position+1)\n",
        "        where relevancy can be 1 or 0\n",
        "    Parameters: \n",
        "    \n",
        "    prediction: ratings for every user predicted by the model\n",
        "    \n",
        "    k: Value for k in Precision@k / Recall@k, to be evaluated for top k recommendations\n",
        "    \n",
        "    threshold: threshold to classify a review as positive or negative, default is 3\n",
        "    \n",
        "    Returns:\n",
        "    A dictionary one for NDCG which is calculated at a user level.\n",
        "    \"\"\"   \n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, act_rating, pred_rating, _ in predictions:\n",
        "        user_est_true[uid].append((pred_rating, act_rating))\n",
        "    dcg = dict()\n",
        "    ## DCG\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        \n",
        "        terms = []\n",
        "        pos = 1\n",
        "        for (pred,act) in user_ratings[:k]:\n",
        "            if ((pred >= threshold) and (act >= threshold)):\n",
        "                terms.append(1/np.log2(pos+1))\n",
        "                pos = pos+1\n",
        "            else:\n",
        "                terms.append(0)\n",
        "                pos = pos+1\n",
        "        dcg[uid] = sum(terms)\n",
        "    #IDCG\n",
        "    idcg = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        terms = []\n",
        "        pos = 1\n",
        "        for (pred,act) in user_ratings[:k]:\n",
        "            if ((act >= threshold)):\n",
        "                terms.append(1/np.log2(pos+1))\n",
        "                pos = pos+1\n",
        "            else:\n",
        "                terms.append(0)\n",
        "                pos = pos+1\n",
        "        idcg[uid] = sum(terms)\n",
        "    ndcg = dict()\n",
        "    for uid in idcg.keys():\n",
        "      ndcg[uid] = dcg[uid]/idcg[uid]\n",
        "    return ndcg"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuEoJw3rYhC5"
      },
      "source": [
        "def calculate_mrr(prediction,k, threshold=3):\n",
        "  \"\"\"\n",
        "  This function calculates the mean reciprocal rank for the predicted ratings of all the users. \n",
        "  Reciprocal rank is the reciprocal of the position at which the first positive recommendation occurs.\n",
        "  For example, if the first recommended restaurant was rated positive by the user, MRR is 1 and if\n",
        "  first recommende was negative but second recommended was positive, then MRR = 1/2 = 0.5\n",
        "  Parameters: \n",
        "\n",
        "  prediction: ratings for every user predicted by the model\n",
        "\n",
        "  k: Value for k in MRR@K, to be evaluated for top k recommendations\n",
        "\n",
        "  threshold: threshold to classify a review as positive or negative, default is 3\n",
        "\n",
        "  Returns:\n",
        "  A list of Reciprocal rank values at user level.\n",
        "  \"\"\"    \n",
        "  user_est_true = defaultdict(list)\n",
        "  for uid, _, act_rating, pred_rating, _ in predictions:\n",
        "      user_est_true[uid].append((pred_rating, act_rating))\n",
        "  reci_rank=[]\n",
        "  for uid, user_ratings in user_est_true.items():\n",
        "      # Sort user ratings by estimated value\n",
        "      user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "      for idx,(pred,act) in enumerate(user_ratings[:k]):\n",
        "          if ((pred >= threshold) and (act >= threshold)):\n",
        "              reci_rank.append(1/(idx+1))\n",
        "              break\n",
        "          else:\n",
        "              continue\n",
        "  return reci_rank"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXE-A1GrV2CS"
      },
      "source": [
        "## precision, recall  and NDCG at k ##\n",
        "def display_all_metrics(predictions,k):\n",
        "    \"\"\"\n",
        "    This function prints all the evaluation metrics.\n",
        "    \n",
        "    Parameters: \n",
        "    \n",
        "    predictions: ratings for every user predicted by the model\n",
        "    \n",
        "    k: Value for k in Precision@k / Recall@k, to be evaluated for top k recommendations\n",
        "    \n",
        "    Returns:\n",
        "    Prints Precision, Recall and NDCG for the provided value of k.\n",
        "    \"\"\"\n",
        "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3)\n",
        "    # ndcg = calculate_ndcg(predictions,k=k,threshold=3)\n",
        "    mrr = calculate_mrr(predictions,k=k,threshold=3)\n",
        "    ## mean precision over all users ##\n",
        "    print('Precision at ',k,':',sum(prec for prec in precisions.values()) / len(precisions))\n",
        "    ## mean recall over all users ##\n",
        "    print('Recall at ',k,':',sum(rec for rec in recalls.values()) / len(recalls))\n",
        "    ## mean NDCG? ##\n",
        "    # print('NDCG at ',k,':',sum(val for val in ndcg.values()) / len(ndcg))\n",
        "    ## mean reciprocal rank ##\n",
        "    print('MRR at ',k,':',np.mean(np.array(mrr)))    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpqMwOPYV2CT"
      },
      "source": [
        "## Base model\n",
        "The train_test_split divides data into test and train but instead of selecting all the ratings of a user, random number of ratings are used as test data.\n",
        "The baseline models only check the user and business biases ie. how far are the user or business ratings from the average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VooRA7n21mM",
        "outputId": "ccf45af3-eae7-444d-cc27-a0bf8fcf4baf"
      },
      "source": [
        "## using a pandas dataframe ##\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(user_review[['user_id', 'business_id', 'user_stars']], reader)\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=.20)\n",
        "bsl_options = {'method': 'als',\n",
        "               'n_epochs': 5,\n",
        "               'reg_u': 12,\n",
        "               'reg_i': 5\n",
        "               }\n",
        "algo = BaselineOnly(bsl_options=bsl_options)\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings for the testset\n",
        "# we can add cross validation later on #\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimating biases using als...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW3CMihn21pc",
        "outputId": "4bf09adc-d758-421a-aef2-3c1e8381ed39"
      },
      "source": [
        "print('RMSE: ',accuracy.rmse(predictions))\n",
        "display_all_metrics(predictions,k=5)\n",
        "display_all_metrics(predictions,k=10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 1.0088\n",
            "RMSE:  1.0087571780681397\n",
            "Precision at  5 : 0.9180761574520186\n",
            "Recall at  5 : 0.36447005704903945\n",
            "MRR at  5 : 0.9718800439021241\n",
            "Precision at  10 : 0.8961652188005568\n",
            "Recall at  10 : 0.6524122975766956\n",
            "MRR at  10 : 0.9717241156726053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xioQjv7u21sV",
        "outputId": "35eef352-89a3-47d5-a50d-7279e9aa2387"
      },
      "source": [
        "## using a pandas dataframe ##\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(user_review[['user_id', 'business_id', 'user_stars']], reader)\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=.20)\n",
        "bsl_options = {'method': 'sgd',\n",
        "               'learning_rate': .00005,\n",
        "               }\n",
        "algo = BaselineOnly(bsl_options=bsl_options)\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings for the testset\n",
        "# we can add cross validation later on #\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimating biases using sgd...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqDc-P7G4PSl",
        "outputId": "bd203a05-6b50-4856-94eb-f46f98b3953f"
      },
      "source": [
        "print('RMSE: ',accuracy.rmse(predictions))\n",
        "display_all_metrics(predictions,k=5)\n",
        "display_all_metrics(predictions,k=10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 1.1110\n",
            "RMSE:  1.1109622997056654\n",
            "Precision at  5 : 0.9173120143838889\n",
            "Recall at  5 : 0.37021841213215906\n",
            "MRR at  5 : 0.9697545144913566\n",
            "Precision at  10 : 0.889247721177992\n",
            "Recall at  10 : 0.6695333809495542\n",
            "MRR at  10 : 0.9694296848950925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzdaeA6t4AVU"
      },
      "source": [
        "# SVD with Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rGVxOudV2CT"
      },
      "source": [
        "## using a pandas dataframe ##\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(user_review[['user_id', 'business_id', 'user_stars']], reader)\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=.20)\n",
        "algo = SVD()\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings for the testset\n",
        "# we can add cross validation later on #\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVoE0WZXV2CT",
        "outputId": "246d79c4-9d4e-49a9-aca8-da16f675efb0"
      },
      "source": [
        "print('RMSE: ',accuracy.rmse(predictions))\n",
        "display_all_metrics(predictions,k=5)\n",
        "display_all_metrics(predictions,k=10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 1.0232\n",
            "RMSE:  1.0231571941956277\n",
            "Precision at  5 : 0.9126565209015799\n",
            "Recall at  5 : 0.35937139116992023\n",
            "MRR at  5 : 0.9719112406867509\n",
            "Precision at  10 : 0.8914748664507952\n",
            "Recall at  10 : 0.6441468140192236\n",
            "MRR at  10 : 0.9711248266296809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdq97rspV2CT"
      },
      "source": [
        "## Using K-fold CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mjp4c8cV2CT"
      },
      "source": [
        "## Using k-fold cross validations ##\n",
        "#data = Dataset.load_builtin('ml-100k')\n",
        "kf = KFold(n_splits=5)\n",
        "algo = SVD()\n",
        "n = 1\n",
        "for trainset, testset in kf.split(data):\n",
        "    print('Fold: ',n)\n",
        "    algo.fit(trainset)\n",
        "    predictions_kfold = algo.test(testset)\n",
        "    accuracy.rmse(predictions_kfold)\n",
        "    display_all_metrics(predictions_kfold,k=15)\n",
        "    display_all_metrics(predictions_kfold,k=10)\n",
        "    n=n+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3CnhYA1V2CT"
      },
      "source": [
        "## Top 10 recommendations for every user ##\n",
        "top_n = get_top_n(predictions, n=10)\n",
        "## Top 10 from k-fold ##\n",
        "top_n_kfold = get_top_n(predictions_kfold, n=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "w2vhha9IV2CT"
      },
      "source": [
        "## GridSearchCV for getting the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXggpQhwV2CT"
      },
      "source": [
        "param_grid = {'n_epochs': [5, 10, 15, 25], 'lr_all': [0.0005, 0.002, 0.005, 0.01, 0.08, 0.1],\n",
        "              'reg_all': [0.01, 0.08, 0.1, 0.5, 0.9]}\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVIUkHIlV2CT"
      },
      "source": [
        "gs.fit(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_2z5if5V2CT"
      },
      "source": [
        "# best RMSE score\n",
        "print(gs.best_score['rmse'])\n",
        "\n",
        "# combination of parameters that gave the best RMSE score\n",
        "print(gs.best_params['rmse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-vDjMoFV2CT"
      },
      "source": [
        "## Testing with an anti-test data\n",
        "Anti-test data would be the businesses/restaurants that a user has not rated yet. Example, if out of 100 restaurants, a user has rated 20, then the ant-test dataset would include the other 80 restaurants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83t3ys-PV2CT"
      },
      "source": [
        "## using a pandas dataframe ##\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(user_review[['user_id', 'business_id', 'user_stars']], reader)\n",
        "\n",
        "trainset = data.build_full_trainset()\n",
        "algo = SVD()\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Testing on anti-test data\n",
        "testset = trainset.build_anti_testset()\n",
        "predictions = algo.test(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp7SOy2tV2CT"
      },
      "source": [
        "top_n_antitest = get_top_n(predictions,10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}